\chapter{Wstęp}

\section{Motywacja}

...

\section{Zawartość pracy}

...


\section{State of the art}

Analiza dużych zbiorów oraz strumieni danych znajduje zastosowanie w dziedzinie Internet Of Things - gdzie kładzie się nacisk na przechowywanie danych i względy bezpieczeństwa \cite{Chui}, reklama - rekomendacje w czasie rzeczywistym \cite{Akter+Wamba}, UX - analiza zachowań użytkowników na stronach internetowych dzięki zbieraniu danych o kliknięciach \cite{Balachandran}. Metodologie i narzędzie związane z analizą strumieni danych są powiązane z uczeniem maszynowym. Model obliczeń powstały dzięki uczeniu maszynowemu może być z powodzenia stosowane do analizy danych prowadzonej w czasie rzeczywistym \cite{Brink+Richards+Fetherolf}.

Metodologie znajdujące zastosowanie w analizie strumieni danych są metodologiami znanymi z dziedziny eksploracji danych. Pierwszą z popularnych metodologii analizy dużych danych jest metoda SEMMA \cite{Semma}. Metodologia ta kładzie nacisk na modelowanie danych, aspekty biznesowe są pomijane. Z powodu pominięcia aspektów praktycznych oraz struktury przypominającej nieco metodę waterfall, SEMMA nie wydaje się być metodologią odpowiednią do pracy z danymi pochądzącymi z urządzeń Internet Of Things.

Kolejna metodologia pracy z Big Data to CRISP-DM \cite{Shearer}. Jest to najpopularniejsze (wg wyników ankiet przeprowadzonych wśród badaczy) podejście do pracy z dużymi zbiorami danych. CRISP-DM uwzględnia zarówno aspekty biznesowe jak i modelowanie. Co więcej, model zakłada ewaluacje oraz przejścia pomiędzy poszczególnymi etapami pracy w zależności od potrzeb. Metodologia ta z uwagi na popularność, systematykę oraz iteracyjność wydaje się być odpowiednia do analizy strumieni danych. CRISP-DM znalazło zastowanie w analizie uporządkowanych strumieni danych do celu predykcji chorób rozwijających się u palaczy \cite{Kalgotra+Sharda}. Opracowanym tam algorytmem można analizować powiązania pomiędzy danymi w strumieniu.

Następna metodologia pracy koncentrującą się na danych pochodzących z Internetu Rzeczy jest metodologia rozszerzonego CRISP-DM \cite{Bernard+Jaokar}. Rozszerzenie to bierze pod uwagę zarówno aspekty związane z infrastrukturą Internet of Things, jak i z samą analizą danych. Ze względu na uwzględnienie problemów związanych z infrastrukturą, metodologia ta nie jest ściśle związana z przetwarzeniem strumieni danych, poprzez to nie można jej traktować jako uniwersalnego sposóbu na ich analizę.

Równie ważnym elementem w analizie strumieni danych co metodologie są narzędzia umożliwiające taką analizę. Narzędziem dedykowanym do pracy ze strumieniami danych jest framework Apache Flink \cite{Flink}. Apache Flink, narzędzie służące do analizy danych w czasie rzeczywistym, znajduje szerokie zastosowanie w świecie big data. Projekt ten wyewoluował z projektu Apache Spark Streaming \cite{SparkStreaming}, który oferuje podobny wachlarz zastosowań co Apache Flink, jednakże jest tylko swego rodzaju nakładką na pracującego na dużych paczkach danych Apache Sparka, poprzez to analiza danych w czasie rzeczywistym jest w nim niemożliwa.

Narzędziem które umożliwa pracę z prawdziwymi strumieniami danych dostarczanym przez Amazon Web Services jest Amazon Kinesis. W oparciu o Amazon Kinesis pokazano jak zbudować dashboard prezentujący w czasie rzeczywistym analizy strumieni danych pochądzących z kliknięć użytkowników na stronie internetowej \cite{Kinesis}. Zaproponowana architektura do przyjmowania, analizy i przetwarzania dużych zbiorów danych miała na celu być jak najbardziej optymalna kosztowo i do zestawienia w prosty sposób. Wadą tego rozwiązania jest to, że Amazon Kinesis jest rozwiązaniem komercyjnym i nie mamy pewności, że przetrwa próbę czasu.