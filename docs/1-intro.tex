\chapter{Wstęp}

\section{Motywacja}

Wraz z postępującą cyfryzacją niemal wszystkich dziedzin ludzkich działalności, gwałtownie zwiększa się ilość produkowanych danych. Szacuje się, że do 2013 roku ludzkość wygenerowała około 4.4 zettabajtów wszelkiego rodzaju danych cyfrowych. Według badaczy, do roku 2020 liczba ta zwiększy się do ponad 44 zettabajtów \cite{EMCCorpReport}. Dane te pochodzą z bardzo różnych źródeł - są wytwarzane przez użytkowników oprogramowania, centra danych, urządzenia internetu rzeczy, sieci sensorów, pojazdy czy też maszyny przemysłowe.  Biorąc pod uwagę ilość danych, a także tempo, w którym są one wytwarzane, szybko można dojść do wniosku, że samo zgromadzenie wszystkich informacji, a także ich późniejsza analiza jest praktycznie niemożliwa, a dane mogą zostać przetworzone albo zaraz po ich wyprodukowaniu, albo nigdy. Z tego powodu na popularności zyskuje paradygmat przetwarzania strumieni danych. Na rynku pojawiają się coraz to nowe narzędzia wspierające wspomniane przed chwilą podejście do przetwarzania danych. Są one dostarczane zarówno w postaci wolnego oprogramowania, np. za pośrednictwem Fundacji Apache, jak i gotowych do szybkiego użycia platform komercyjnych. Nie została jednak wypracowana standardowa metodologia pracy ze strumieniami danych, która wyznaczałaby etapy konieczne do zbudowania systemu pozwalającego na zgromadzenie danych, zrozumienie ich, analizę, a także wizualizację wyników. Stworzenie takiej metodologii stanowi motywację poniższej pracy.

\section{Strumienie danych}

Strumieniem danych nazywamy potencjalnie nieskończony, uporządkowany ciąg krotek generowany przez jedno lub wiele urządzeń jednocześnie. Poszczególne krotki mogą, lecz nie muszą, być od siebie zależne lub być skorelowane. Rozmiar pojedynczej krotki zazwyczaj jest niewielki (rząd KB). Ilość danych w strumieniu najczęściej jest jednak bardzo duża, ze względu na ilość źródeł oraz tempo, w jakim dane są wytwarzane. Przykładami strumieni danych są np. strumień aktywności użytkowników sieci społecznościowych, odczyty aparatury monitorującej kondycję wałów przeciwpowodziowych, czy też wpisy w logach serwerów w centrach danych. Z pracą z danymi strumieniowymi wiąże się szereg problemów niewystępujących podczas konwencjonalnej analizy danych statycznych. Ze względu na ilość danych pojawiających się w systemie w każdej sekundzie, niemożliwe jest zapisanie ich w hurtowni danych. Konieczne jest przetworzenie każdej tupli ad hoc, co pociąga za sobą restrykcyjne wymagania co do odporności na awarie. Przykładem innego problemu związanego z przetwarzaniem strumieni danych jest konieczność uwzględnienia w obliczeniach krotek, które dostarczane są z opóźnieniem, np. ze względu na awarię źródła czy też opóźnienia w infrastrukturze przesyłającej dane.

\section{Definicja problemu}



\section{Cel pracy}

Celem pracy jest opracowanie kompleksowej metodologii pracy z danymi strumieniowymi. Metodologia zostanie wypracowana podczas budowy systemu filtrującego strumień aktywności użytkowników serwisu Twitter z wykorzystaniem algorytmu uczenia maszynowego. Będzie ona poruszać problemy związane z przygotowaniem architektury systemu odpowiedniej do przetwarzania strumieni danych dostarczanych do systemu z dużą szybkością, zbieraniem reprezentatywnych próbek danych wymaganych do modelowania, stworzeniem modelu uczenia maszynowego oraz z przedstawieniem wyników w czasie rzeczywistym. Dla każdego z etapów metodologii, zasugerowane zostaną narzędzia Open Source pozwalające na realizację poszczególnych etapów.
